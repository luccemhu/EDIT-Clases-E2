---
title: "**<b style = 'color : #E34B2A;'>Análisis de Variables Latentes</b>**"
subtitle: 'Profesor: <a href="http://www.pucp.edu.pe/profesor/jose-manuel-magallanes/" target="_blank">Dr. José Manuel MAGALLANES REYES, Ph.D.</a>'
date: "2023-07-26"
author: "Editors: [Gianfranco Romero](https://github.com/GianfrancoRomero), \n a20196091@pucp.edu.pe & \n [Joel Hu](https://github.com/luccemhu), \n a20196510@pucp.edu.pe"
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
    self_contained: true
    thumbnails: false
    lightbox: true
    gallery: false
    highlight: github
    code_folding: "show"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```
<a id='beginning'></a>

Muchas veces queremos saber si algun conjunto de variables representa algun *concepto*, al cual se le denomina técnicamente *variable latente*. Las técnicas son variadas, pero aquí aplicaremos análisis factorial: el exploratorio y el confirmatorio para tratar de *reducir* varias variables en otra u otras más simples.

````{=html}
<!--

-->
````
# Preprocesamiento (Revisar el `.rmd` sobre este paso):



Para esta sesión trabajaremos con la data de:

-   [Índice de Desarrollo Humano](https://es.wikipedia.org/wiki/Anexo:Pa%C3%ADses_por_%C3%ADndice_de_desarrollo_humano)

-   [Índice de Democracia](https://es.wikipedia.org/wiki/%C3%8Dndice_de_democracia)

## Carga de datos

Para esta etapa vamos a proceder a *scrapear* las dos páginas web, con la ayuda de la biblioteca *htmltab*. La descarga utilizará el *xpath* de la tabla de interés.

```{r, echo=TRUE, eval=TRUE,warning=FALSE, message=FALSE}
rm(list = ls())

library(htmltab)

# links
WhereIDH=list(page="https://es.wikipedia.org/wiki/Anexo:Pa%C3%ADses_por_%C3%ADndice_de_desarrollo_humano", 
              xpath='//*[@id="mw-content-text"]/div[1]/table[2]/tbody') # Esto se actualiza
WhereDEMO=list(page="https://es.wikipedia.org/wiki/%C3%8Dndice_de_democracia",
               xpath='//*[@id="mw-content-text"]/div[1]/div[2]/div/table/tbody') # Esto se actualiza

#carga
idh  = htmltab(doc = WhereIDH$page, 
               which  = WhereIDH$xpath,
               encoding = "UTF-8")
demo  = htmltab(doc = WhereDEMO$page, 
               which  = WhereDEMO$xpath,
               encoding = "UTF-8")
```

## Limpieza

Por lo general, la data scrapeada presenta diversas 'impurezas'. Veámos qué se necesita 'limpiar'.

-   **Nombres de columnas**: Hay que tratar de tener nombres simples, sin espacios ni caracteres especiales. Si se desea mantener nombres descriptivos, puede hacerse uso de guiones bajos (underscores) o formato *CamelCase*.

Los nombres de la data *idh* son muy largos, con caracteres especiales, en español, y muchos espacios en blancos.

```{r}
names(idh)
```

Los nombres de la data *demo* no son muy largos, con caracteres en español, y muchos espacios en blancos.

```{r}
names(demo)
```

Por lo visto, hay que cambiar los nombres en ambas tablas.

```{r}
# en IDH
## cambio total
newNames = c('Pais','EsperanzaVida','EscolaridadDuracion','EscolaridadPromedio','PBI')
names(idh) = newNames

# en DEMO
## Capitalizar
library(stringr)
names(demo) = str_to_title(names(demo))

## sin tildes ni ñs.
library(stringi)
names(demo) = stri_trans_general(str = names(demo),
                                 id = "Latin-ASCII")
## sin espacios
names(demo) = gsub(" ", "", names(demo))
```

-   **Valores en las celdas**: Por lo general, hay que asegurarse que no haya espacios en blanco ni al inicio ni al final de cada valor en una celda.

```{r}
idh[, ] = lapply(idh[, ], trimws, whitespace = "[\\h\\v]")

demo[, ] = lapply(demo[, ], trimws, whitespace = "[\\h\\v]") 
```

## Formateo

Hablamos de formateo cuando buscamos que los valores de cada celda estén el correcto tipo de dato. Para ello debemos primero ver qué tipo ha sido asignado por R.

```{r}
str(idh)
```

```{r}
str(demo)
```

-   **Conversión a tipo numérico**: Vemos que muchos valores que deberian ser numéricos han sido reconocidos como texto. Normalmente eso sucede pues hay caracteres dentro del numero que evitan que se lea adecuadamente. Luego de esa corrección recién se puede cambiar el tipo.

```{r}
# eliminar coma en los miles:
idh$PBI=gsub(',','',idh$PBI)
# ahora a numerico
idh[,-1]=lapply(idh[,-1], as.numeric)

# cambiar coma en los decimales:
demo[,-c(2,9)]=lapply(demo[,-c(2,9)],
                      function(x){gsub(",",".",x)})
# ahora a numerico
demo[,-c(2,9)]=lapply(demo[,-c(2,9)], as.numeric)
```

Luego de pasar a tipo numérico, las celdas que no tenían un valor numérico adecuado se convirtieron en NAs. Aquí hay que revisar las filas donde eso se generó.

```{r}
idh[!complete.cases(idh),]
```

```{r}
demo[!complete.cases(demo),]
```

A partir de lo visto, decidir si se puede completar los valores faltantes. Luego, ya nos quedamos con los datos completo.

```{r}
##
idh[idh$Pais=='Camerún','EscolaridadDuracion']=13.1
demo[demo$Puesto==48 & !is.na(demo$Puesto),'Pais']='Panama'

##
idh=idh[complete.cases(idh),]
demo=demo[complete.cases(demo),]
```

-   **Caracteres de Alfabeto español**: Es preferible eliminarlos.

```{r}
# sin tildes
idh$Pais=stri_trans_general(str = idh$Pais, 
                               id = "Latin-ASCII")

demo[,c(2,9)]=lapply(demo[,c(2,9)],
                     stri_trans_general,
                     id = "Latin-ASCII") 
```

## Merge

-   **Verificando qué falta en el campo clave**: El merge usa columnas comunes. Antes del merge definitivo hay que verificar si hay correcciones posible para que el merge no pierda tantas filas. 

Una manera práctica para darnos cuenta que NO está coincidiendo en dos conjuntos es usar diferencia de conjuntos ^[Sí A y B son conjuntos, A−B serán los elementos que están en A pero que NO están en B, por ejemplo si A={1,2,3} y B={3,5}, entonces A−B={1,2}.]:

```{r}
setdiff(demo$Pais,idh$Pais)
```

De igual manera:

```{r}
setdiff(idh$Pais,demo$Pais)
```

Se puede corroborar que sí hay valores que pueden ser corregidos.

```{r}
demo[demo$Pais=='Republica de China','Pais']='China'
demo[demo$Pais=='R. Democratica del Congo','Pais']='Republica Democratica del Congo'

```

Ahora si hay más comodidad para hacer el merge:

```{r}
idhdemo=merge(idh,demo)
head(idhdemo)
```

Tenemos un data frame que integra diversas variables que quieren medir conceptos complejos (latentes). Vemos cómo usamos el análisis factorial.


# Analisis Factorial Exploratorio (EFA)

- Explora la data y nos entrega posibles factores que resúmen cada uno un conjunto de variables.

Pasos que el EFA requiere:

1.  Subsetear la data

```{r}
dontselect = c("Pais", "Puesto", "Puntuacion", 'Categoria')
select = setdiff(names(idhdemo), dontselect)
theData = idhdemo[, select]
```

2.  Calculo de matriz de correlación:

```{r, echo=TRUE, eval=TRUE,warning=FALSE, message=FALSE}
# Polycor halla la relación policórica: funciona con variables categóricas y numéricas 
# Cuando solo se usa con numéricas, calcula el pearson, la correlación.
library(polycor)
corMatrix=polycor::hetcor(theData)$correlations
```

```{r, eval=FALSE}
La correlación es una medida estadística que indica la relación o grado de asociación entre dos variables. Se utiliza para determinar si existe una relación lineal entre las variables y en qué medida se mueven juntas. La correlación se representa por un coeficiente de correlación, que puede variar entre -1 y 1.


Un coeficiente de correlación de 1 indica una correlación positiva perfecta, lo que significa que las dos variables tienen una relación lineal directa y se mueven en la misma dirección. Por ejemplo, si una variable aumenta, la otra también lo hace en la misma proporción.


Un coeficiente de correlación de -1 indica una correlación negativa perfecta, lo que significa que las dos variables tienen una relación lineal inversa y se mueven en direcciones opuestas. Por ejemplo, si una variable aumenta, la otra disminuye en la misma proporción.


Un coeficiente de correlación cercano a 0 indica una correlación débil o nula, lo que significa que no hay una relación lineal clara entre las variables.


Es importante destacar que la correlación no implica causalidad. Solo porque dos variables estén correlacionadas no significa necesariamente que una variable cause el cambio en la otra. Puede existir una relación espuria o la influencia de una tercera variable que afecte a ambas.


Existen diferentes métodos para calcular el coeficiente de correlación, siendo el más común el coeficiente de correlación de Pearson. También existen otros coeficientes de correlación, como el coeficiente de correlación de Spearman, que se utiliza cuando las variables no tienen una relación lineal, o el coeficiente de correlación de Kendall, que se utiliza para variables ordinales.


En resumen, la correlación es una medida estadística que permite cuantificar la relación entre dos variables y proporciona información sobre cómo se mueven juntas.
```

3.  Explorar correlaciones... entre todas las variables a utilizar:

- 1er paso para comprobar las variables latentes con la teoria:
```{r coorPlot, echo=TRUE, eval=TRUE,warning=FALSE, message=FALSE, fig.cap="Matriz de Correlaciones"}

library(ggcorrplot)
ggcorrplot(corMatrix)
```

- Si puedes ver bloques correlacionados, hay esperanza de un buen analisis factorial.** Mientras más fuerte sea el color, mayor será la correlación** (en esta grafica, en otras se puede indicar de otro modo). La diagonal se ignora pues compara una variable con sí misma. 
**Si no hay correlación, se ha armado un mal índice**. Se espera que las variables de un concepto esten muy correlacionadas: INPUT para la EFA. Matriz de correlacion (algebra)

- **Todas las tecnicas de factoriales depende de la matriz de correlacion.**

  - EN SUMA: EFA: DATA Y MATRIZ DE CORRELACIÓN


4.  Verificar si los datos permiten factorizar: 

- KMO(corMatrix) <- Overall MSA > 0.6 (lo contrario sugiere cambiar esas variables)
```{r, echo=TRUE, eval=TRUE,warning=FALSE, message=FALSE}
library(psych)
psych::KMO(corMatrix) 
# Busca saber si la data se adecua a un análisis factorial. 
# El Overall MSA en este caso es 0.90, lo cual es aceptable. 
# Generalmente, se aceptan MSA mayores a 0.6 
# (lo contrario sugiere cambiar esas variables) en CC.SS. 
# KMO relacionado con el tamano de los datos es suficiente 
# para comenzar con el EFA, respecto a la data.
```

5.  Verificar si la matriz de correlaciones es adecuada

Aqui hay dos pruebas:

-   Hnula: La matriz de correlacion es una [matriz identidad](https://en.wikipedia.org/wiki/Identity_matrix)

```{r, echo=TRUE, eval=TRUE,warning=FALSE, message=FALSE}
cortest.bartlett(corMatrix, n = nrow(theData))$p.value > 0.05
```

-   Hnula: La matriz de correlacion es una [matriz singular](http://mathworld.wolfram.com/SingularMatrix.html).

```{r, echo=TRUE, eval=TRUE,warning=FALSE, message=FALSE}
library(matrixcalc)

is.singular.matrix(corMatrix)
```

6.  Determinar en cuantos factores o variables latentes podríamos redimensionar la data: En este caso, la función *fa.parallel* nos dará la sugerencia:

```{r, echo=TRUE, eval=TRUE,warning=FALSE, message=FALSE}
fa.parallel(theData, fa = 'fa', correct = T, plot = F)
```

Se sugieren 2, lo esperado, sigamos.

7.  Redimensionar a número menor de factores

-   Resultado inicial:

```{r, echo=TRUE, eval=TRUE,warning=FALSE, message=FALSE}
library(GPArotation)
resfa <- fa(theData,
            nfactors = 2,
            cor = 'mixed',
            rotate = "varimax",
            fm = "minres")
print(resfa$loadings)
```

-   Resultado mejorado (solo apropiado si hay más de un factor):

```{r, echo=TRUE, eval=TRUE,warning=FALSE, message=FALSE}
print(resfa$loadings, cutoff = 0.5)
```

Cuando logramos que cada variable se vaya a un factor, tenemos una *estructura simple*.

-   Resultado visual: El resultado lo podemos ver de manera gráfica en la Figura \@ref(fig:faDiagram).

```{r faDiagram, echo=TRUE, eval=TRUE,warning=FALSE, message=FALSE, fig.cap="Variables organizadas en Factores"}

fa.diagram(resfa,main = "Resultados del EFA")
```

8.  Evaluando Resultado obtenido:

-   ¿Qué variables aportaron mas a los factores?

```{r}
sort(resfa$communality)
```

-   ¿Qué variables contribuyen a más de un factor?

```{r}
sort(resfa$complexity)
```

9.  Valores proyectados: Podemos calcular dos *indices* que resuman los dos factores encontrados.

```{r, echo=TRUE, eval=TRUE,warning=FALSE, message=FALSE}
library(magrittr)
as.data.frame(resfa$scores) |> head()
```

Les daremos por nombre 'demos_efa' y 'desahu_efa' a esas dos columnas. Dado que tenemos el indice de democracia en la data original, comparémoslo con el recién calculado, via el *scatterplot* de la Figura \@ref(fig:scatterEFAdemo1).

```{r scatterEFAdemo1, warning=FALSE, message=FALSE,fig.cap="Comparando Indice de Democracia con el Score obtenido en EFA"}

idhdemo$demos_efa = resfa$scores[, 1]
idhdemo$desahu_efa = resfa$scores[, 2]

ggplot(data = idhdemo, aes(x = Puntuacion, y = demos_efa)) + 
  geom_point() + theme_minimal() + 
  labs(x = "Indice de Democracia (original)", y = "Indice de Democracia EFA")
```

Nota que los rangos de los valores en la Figura \@ref(fig:scatterEFAdemo1) no son los mismos. La Figura \@ref(fig:scatterEFAdemo2) muestra tales cambios.

```{r scatterEFAdemo2,fig.cap="Comparación Indice de Democracia con Score EFA con rangos coincidentes"}
library(BBmisc)
efa_scores_ok = normalize(resfa$scores,
                        method = "range",
                        margin = 2, # by column
                        range = c(0, 10))

idhdemo$demos_efa_ok = efa_scores_ok[, 1]
idhdemo$desahu_efa_ok = efa_scores_ok[, 2]

ggplot(data = idhdemo, aes(x = Puntuacion, y = demos_efa_ok)) + 
  geom_point() + theme_minimal() + 
  labs(x = "Indice de Democracia (original)", 
       y = "Indice de Democracia EFA (cambiado)")
```

# Análisis Factorial Confirmatorio

El análisis factorial confirmatorio (CFA) lo usamos cuando ya tenemos una teoría y queremos confirmar que los datos pueden reflejar los conceptos o variables latentes asumidas [@costa_confirmatory_nodate,@orcan_exploratory_2018].

```{r}
modelCFA <- ' democracia  =~ ProcesoElectoralyPluralismo + FuncionamientodelGobierno + Participacionpolitica + Culturapolitica + Derechosciviles

desaHumano=~EsperanzaVida+EscolaridadDuracion+EscolaridadPromedio+PBI'
```

Ahora vemos qué arroja el modelo:

```{r}
# normalizar las variables:
theDataNorm = scale(theData)

library(lavaan)
cfa_fit <- cfa(modelCFA, data = theDataNorm,
               std.lv = TRUE,
               missing = "fiml")
summary(cfa_fit)
```

Averigüemos qué tan bien salió el modelo:

```{r}
allParamCFA = parameterEstimates(cfa_fit, standardized = T)
allFitCFA = as.list(fitMeasures(cfa_fit))
```

-   El ChiSquare es NO significativo? (p_value debe ser mayor a 0.05 para que sea bueno)

```{r}

allFitCFA[c("chisq", "df", "pvalue")] # pvalue>0.05
```

-   El Índice Tucker Lewis es mayor a 0.9?

```{r,echo=TRUE}
allFitCFA$tli 
```

-   La Raíz del error cuadrático medio de aproximación es menor a 0.05?

```{r,echo=TRUE}
allFitCFA[c('rmsea.ci.lower', 'rmsea' , 'rmsea.ci.upper')] 
```

Ya sabemos que las latentes no cumplen a cabalidad los requisitos, pero aún así calculamos las puntuaciones obtenidas por esta vía.

```{r}
scorescfa = normalize(lavPredict(cfa_fit),
                    method = "range",
                    margin = 2, # by column
                    range = c(0, 10))

idhdemo$demos_cfa_ok = scorescfa[, 1]
idhdemo$desahu_cfa_ok = scorescfa[, 2]
```

Veamos que tanto se parece el score obtenido via CFA con la puntuación original en la Figura \@ref(fig:scatterCFAdemo).

```{r scatterCFAdemo,fig.cap="Comparación Indice de Democracia con Score CFA con puntuación original"}
ggplot(data = idhdemo, aes(x = Puntuacion, y = demos_cfa_ok)) + 
  geom_point() + theme_minimal() + 
  labs(x = "Indice de Democracia (original)", 
       y = "Indice de Democracia CFA (cambiado)")
```

Podemos ver los resultados del CFA en la Figura \@ref(fig:cfaPlot).

```{r cfaPlot, fig.cap="Representación del CFA - Democracia y IDH"}
library(lavaanPlot)
lavaanPlot(model = cfa_fit, 
           node_options = list(shape = "box", fontname = "Helvetica"), 
           edge_options = list(color = "grey"), coefs = T)

```

Podríamos hacer una regresión donde una de estas latentes es la dependiente y la otra la independiente. El siguiente paso en esta línea sería usar un modelo de **Ecuaciones Estructurales**. Primero usemos la regresión convencional, encontrando estos resultados:

```{r}
hipotesis = formula(demos_cfa_ok ~ desahu_cfa_ok)
reg1 = lm(hipotesis, data = idhdemo)
summary(reg1)
```

Ahora, usando variables latentes en una Ecuación Estructural.

```{r}

modelSEM <- ' democracia  =~ ProcesoElectoralyPluralismo + FuncionamientodelGobierno + Participacionpolitica + Culturapolitica + Derechosciviles

desaHumano=~EsperanzaVida+EscolaridadDuracion+EscolaridadPromedio+PBI

democracia~desaHumano'

```

Los resultados son:

```{r, warning=FALSE}
sem_fit <- sem(modelSEM,
               data = theDataNorm)
summary(sem_fit)
```

El resultado podemos verlo de manera gráfica en la Figura \@ref(fig:semPlot1).

```{r semPlot1, fig.cap="SEM con Democracia como dependiente e IDH como independiente"}

lavaanPlot(model = sem_fit,
           node_options = list(shape = "box",
                               fontname = "Helvetica"),
           edge_options = list(color = "grey"), coefs = T, stand = T)
```

El mismo resultado podemos verlo de manera gráfica usando otra la biblioteca *semPlot* en la Figura \@ref(fig:semPlot2).


```{r semPlot2, fig.cap="SEM alternativo con Democracia como dependiente e IDH como independiente"}
library(semPlot)
semPaths(sem_fit, residuals = F,
         sizeMan = 7, sizeLat = 12,
         what = "std",
         nCharNodes = 10,
         posCol = c("skyblue4", "red"),
         edge.color = "orange",
         edge.label.cex = 1.2, layout = "circle2")
```

<br></br> <br></br> [Comienzo](#beginning) <br></br> <br></br>