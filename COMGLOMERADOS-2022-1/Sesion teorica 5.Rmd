---
title: "Análisis de conglomerados"
author: "Gianfranco Raúl Romero Sucapuca"
date: '2022-05-27'
output: html_document
---

# Estrategia de partición

Los **conglomerados** son grupos cuyos elementos son homogéneos. Esta homogeneidad los diferencia de otros conglomerados. Un elemento es tal que estáen un conglomerado y no en otro.

Como su nombre lo indica, la estrategia de partición busca partir los casos en grupos. El algoritmo básico establece puntos que deben atraer a los casos, tal que estos se separen. Claro está, que estos puntos atractores van moviendose conforme los grupos se van formando, hasta que al final se han partido todos los casos.

Hay diversos algoritmos que buscan una implementación de estos principios básicos. El más conocido es el de **K-medias**, pero para ciencias sociales tiene la desventaja que requiere que todas las variables sean numéricas, no siendo muy adecuado ante categorías. Es decir, la técnica de k-means no usa distancias entre categóricas, sólo entre numéricas (la distancia Euclideana).

La alternativa a las necesidades en ciencias sociales es la técnica de **k-medoides**. Esta técnica trabaja muy bien con las distancias euclideas, pero también con otras distancias como la Mahattan (revisar este debate). En particular, usaremos la distancia Gower útil para todos tipo de escalas.

# Parte I: Preparación

1. Links de la data

```{r}
# borrando todo:
rm(list = ls())

### 
linkEDUgdp="https://www.cia.gov/the-world-factbook/field/education-expenditures/country-comparison"
linkMILIgdp="https://www.cia.gov/the-world-factbook/field/military-expenditures/country-comparison"
linkKWHprod="https://www.cia.gov/the-world-factbook/field/electricity-production/country-comparison"
```

2. Descarga desde la web

2.1 Primero los paths

```{r}
EDUpath='//*[@id="index-content-section"]/div/div[2]/div/div/div/div/div/table'
MILIpath='//*[@id="index-content-section"]/div/div[2]/div/div/div/div/div/table'
KWHpath = '//*[@id="index-content-section"]/div/div[2]/div/div/div/div/div/table'
```

2.2 Scrapping

```{r}
library(htmltab)
edu<- htmltab(doc = linkEDUgdp,
              which =EDUpath)

mili<- htmltab(doc = linkMILIgdp,
              which =MILIpath)

elec<- htmltab(doc = linkKWHprod,
              which =KWHpath)
```

3. Integrado los datos

Tenemos tres tablas, con la misma unidad de análisis (países). Pasemos a integrarlas en una sola. Identifiquemos el campo común (la “key”):

```{r}
list(names(edu), names(mili), names(elec))
```

Vemos que hay campos necesarios e innecesarios (para nuestro propósito simple del momento-pueden no serlo en otra ocasión). Quedémonos con los que nos interesan:

```{r}
keep=c(2,3)
edu=edu[,keep]
mili=mili[,keep]
elec=elec[,keep]
```

Nota que es preferible que la variable a añadir tenga un nombre claro y diferenciado, hagamos el cambio de una vez (la key si debe mantenerse igual):

```{r}
names(edu)[2]="edu_gdp"
names(mili)[2]="mili_gdp"
```

Ahora sí, el **merge** es más sencillo:

```{r}
allData=merge(edu,mili)
allData=merge(allData,elec)
```

Veamos que estructura tenemos:

```{r}
str(allData)
```

Vemos que R ha interpretado los valores como texto. Esto es un problema grave que se debe resolver de inmediato:

```{r}
library(readr)
allData[,-1]=lapply(allData[,-1], parse_number) #Convierte las columnas seleccionadas en numéricas
```

Ahora que sabemos que tenemos números, pasemos a describirlos estadísticamente:

```{r}
summary(allData)
```

5. Verificando distribución (y posible transformación)

Noten que los valores de kWh son muy distintos a los demás. Es muy común que tengamos diferentes unidades, por lo que debemos transformar los datos para evitar confundir a los algoritmos de conglomeración:

```{r}
boxplot(allData[,-1])
```

```{r}
library(BBmisc)

boxplot(normalize(allData[,-1],method='range',range=c(0,1)))
```

```{r}
boxplot(normalize(allData[,-1],method='standardize')) #Variables estandarizadas
```

Nos quedaremos con la última opción:

```{r}
allData[,-1]=normalize(allData[,-1],method='standardize')
allData=allData[complete.cases(allData),]

#descriptivos:
summary(allData)
```

4. Veamos correlaciones

```{r}
cor(allData[,-1])
```

Nótese que la data de educación se correlaciona negativamente. El valor es muy cercano a cero, pero practiquemos cambio de monotonia:

```{r}
allData$edu_gdp=-1*allData$edu_gdp
#ahora:
cor(allData[,-1]) #La correlación entre edu_gdp se vuelve "por cada tanto que NO se invierta en educación, se invierte más en..." #Se necesitan correlaciones positivas #La multiplicación por -1 solo es posible cuando se han estandarizado las variables
```

5. Preparemos la data para la clusterización

No debemos usar los nombres en la clusterización (columna), pero tampoco debemos perderlos:

```{r}
dataClus=allData[,-1]
row.names(dataClus)=allData$Country #Los nombres de los países se vuelven solo los nombres de las filas, pero ya no son una columna como tal
```

# Parte II: Proceso de clusterización

## 1. Calcular distancias entre los casos (países)

```{r}
library(cluster)
g.dist = daisy(dataClus, metric="gower") #Daisy se usa para distancias entre números y categorías
```

# 2. Proponer cantidad de clusters

Pidamos cuatro grupos (de donde salió?) y creemos nueva columna con el identificador:

```{r}
set.seed(123)
pam.resultado=pam(g.dist,4,cluster.only = F) #Aquí el programa creó los clusters cono sus respectivos medoides

#nueva columna
dataClus$pam=pam.resultado$cluster
```

# 3. Explorar resultados

Aquí corresponde saber las caracteristicas de los paises en cada cluster. Veamos el resultado preliminar al aplicar **aggregate**:

```{r}
aggregate(.~ pam, data=dataClus,mean) #Promedio de los valores que pertenecen a cada cluster (grupo)
```

¿Hay que recodificar la etiqueta del cluster?

```{r}
original=aggregate(.~ pam, data=dataClus,mean)
original[order(original$edu_gdp),] #Ordenado de menor a mayor en base al gasto en educación
```

Aqui estamos tomando la decisión de recodificar (aunque la data no lo sugiere claramente):

```{r}
# adaptar
dataClus$pam=dplyr::recode(dataClus$pam, `4` = 1, `2`=2,`1`=3,`3`=4)
```

# Estrategia jerárquica

La jerarquización busca clusterizar por etapas, hasta que todas las posibilidades de clusterizacion sean visible. Este enfoque tiene dos familias de algoritmos:

-Aglomerativos

-Divisivos

# Estrategia aglomerativa

En esta estrategia se parte por considerar cada caso (fila) como un cluster, para de ahi ir creando miniclusters hasta que todos los casos sean un solo cluster. El proceso va mostrando qué tanto esfuerzo toma juntar los elementos cluster tras cluster.

Pasos:

Como ya tenemos distancias entre casos, seguimos:

## 1. Decidir linkages

Esta es la distancia entre los elementos, tenemos que decidir como se irá calculando la distancia entre los clusters que se van formando (ya no son casos individuales). Los tres mas simples metodos:

-Linkage tipo SINGLE.

-Linkage tipo COMPLETE.

-Linkage tipo AVERAGE

Otro metodo adicional, y muy eficiente, es el de **Ward**. Al final, lo que necesitamos saber cual de ellos nos entregará una mejor propuesta de clusters. Usemos este para nuestro caso.

## 2. Calcular clusters

La función **hcut** es la que usaremos para el método jerarquico, y el algoritmo aglomerativo se emplea usando **agnes**. El linkage será **ward** (aquí ward.D):

```{r}
set.seed(123)
library(factoextra)

res.agnes<- hcut(g.dist, k = 4,hc_func='agnes',hc_method = "ward.D") #Agnes se usa para el algoritmo aglomerativo

dataClus$agnes=res.agnes$cluster
```

## 3. Explorar resultados

```{r}
aggregate(.~ agnes, data=dataClus,mean)
```

¿Hay que recodificar la etiqueta del cluster?:

```{r}
original=aggregate(.~ agnes, data=dataClus,mean)
original[order(original$edu_gdp),]
```

Sigamos la estrategia anterior (solo como ejemplo de recodificación):

```{r}
dataClus$agnes=dplyr::recode(dataClus$agnes, `2` = 1, `4`=2,`1`=3,`3`=4)
```

## 4. Visualizar

El **dendograma** nos muestra el proceso de conglomeración:

```{r}
# Visualize
fviz_dend(res.agnes, cex = 0.7, horiz = T)
```

El eje ‘Height’ nos muestra el “costo” de conglomerar.

# Comparando

Veamos qué tanto se parece a la clasificación jerarquica a la de partición:

```{r}
# verificar recodificacion
table(dataClus$pam,dataClus$agnes,dnn = c('Particion','Aglomeracion')) #Se ven las coincidencias entre los clusters de los diferentes métodos #Se debe prestar atención a la diagonal que compara los clusters de misma denominación (1-1;2-2;3-3;4-4)
```

# Estrategia divisiva

Esta estrategia comienza con todos los casos como un gran cluster; para de ahi dividir en clusters más pequeños (obviemos paso “1” anterior):

## 2. Calcular clusters

La función **hcut** es la que usaremos para el método jerarquico, y el algoritmo divisivo se emplea usando **diana**:

```{r}
set.seed(123)
res.diana <- hcut(g.dist, k = 4,hc_func='diana')
dataClus$diana=res.diana$cluster
```

## 3. Explorar resultados

```{r}
aggregate(.~ diana, data=dataClus,mean)
```

¿Hay que recodificar la etiqueta del cluster?

```{r}
original=aggregate(.~ diana, data=dataClus,mean)
original[order(original$edu_gdp),]
```

Igual que antes:

```{r}
dataClus$diana=dplyr::recode(dataClus$diana, `3` = 1, `4`=2,`2`=3,`1`=4)
```

## 4. Visualizar

El **dendograma** nos muestra el proceso de conglomeración:

```{r}
# Visualize
fviz_dend(res.diana, cex = 0.7, horiz = T)
```

El eje ‘Height’ nos muestra el “costo” de conglomerar.

# Comparando

Veamos qué tanto se parece ambas la clasificaciones jerárquicas:

```{r}
# verificar recodificacion
table(dataClus$diana,dataClus$agnes,dnn = c('Division','Aglomeracion'))
```

Estos delata que quizá debimos recodificar por una variable diferente a educación.

Nota que en estas técnicas (partición y jerarquica) todo elemento termina siendo parte de un cluster.

# Estrategia basada en densidad

La estrategia basada en densidad sigue una estrategia muy sencilla: juntar a los casos cuya cercanía entre sí los diferencia de otros.

El algoritmo **dbscan** requiere dos parametros:

1. La distancia epsilon a usar para clusterizar los casos.

2. La cantidad k minima de puntos para formar un cluster. El valor k que se usará es al menos la cantidad de dimensiones (en el caso reciente usaremos k=3).

## Mapa de casos

Sin embargo, el principal problema es que necesitamos un **mapa de posiciones** para todos los casos. Eso requiere una técnica que proyecte las dimensiones originales en un plano bidimensional. Para ello usaremos la técnica llamada **escalamiento multidimensional**:

```{r}
proyeccion = cmdscale(g.dist, k=2,add = T) # k es la cantidad de dimensiones #cmdscale lo convierte en un mapa
```

Habiendo calculado la proyeccción, recuperemos las coordenadas del mapa del mundo basado en nuestras dimensiones nuevas:

```{r}
# data frame prep:
dataClus$dim1 <- proyeccion$points[,1]
dataClus$dim2 <- proyeccion$points[,2]
```

Aquí puedes ver el mapa:

```{r}
base= ggplot(dataClus,aes(x=dim1, y=dim2,label=row.names(dataClus))) 
base + geom_text(size=2)
```

Coloreemos el mapa anterior segun el cluster al que corresponden.

Procedeamos a gráficar:

PAM:

```{r}
base= ggplot(dataClus,aes(x=dim1, y=dim2)) +  coord_fixed()
base + geom_point(size=2, aes(color=as.factor(pam)))  + labs(title = "PAM") 
```

AGNES:

```{r}
base + geom_point(size=2, aes(color=as.factor(agnes))) + labs(title = "AGNES")
```

DIANA

```{r}
base + geom_point(size=2, aes(color=as.factor(diana))) + labs(title = "DIANA")
```

Ahora calculemos usando **dbscan**:

1. Nuevas distancias: Las posiciones son la información para dbscan.

```{r}
# euclidea!!
g.dist.cmd = daisy(dataClus[,c('dim1','dim2')], metric = 'euclidean')
```

2. Calculo de epsilon

```{r}
library(dbscan)
kNNdistplot(g.dist.cmd, k=3)
```

3. Obteniendo clusters

```{r}
library(fpc)
db.cmd = fpc::dbscan(g.dist.cmd, eps=0.065, MinPts=3,method = 'dist')
```

De lo anterior podemos saber:

```{r}
db.cmd
```

-Que se han obtenido 6 clusters

-Que hay 7 elementos que no se pudieron clusterizar

Pongamos esos valores en otra columna:

```{r}
dataClus$db=as.factor(db.cmd$cluster)
```

4. Graficando

Aquí sin texto:

```{r}
library(ggrepel)
base= ggplot(dataClus[dataClus$db!=0,],aes(x=dim1, y=dim2)) + coord_fixed()

dbplot= base + geom_point(aes(color=db)) 

dbplot + geom_point(data=dataClus[dataClus$db==0,],
                    shape=0) 
```

Nota que en esta técnica hay casos que no serán clusterizados.

Finalmente, te recomiendo que intentes cambiar algunos pasos, y tratar de encontrar resultados alternativos.

