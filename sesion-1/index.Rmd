---
title: "**<b style = 'color : #E34B2A;'>Regresión Lineal Multivariada (I)</b>**"
subtitle: ''
date: "2023-07-24"
author: "Gianfranco Romero & Joel Hu | \n **[@GianfrancoRomero](https://github.com/GianfrancoRomero)** \n a20196091@pucp.edu.pe & \n **[@luccemhu](https://github.com/luccemhu)** \n a20196510@pucp.edu.pe"
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
    self_contained: true
    thumbnails: false
    lightbox: true
    gallery: false
    highlight: github
    code_folding: "show"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

<a id='beginning'></a>

I. De Correlación a regresión
============================================================

- Utilizaremos la técnica de la regresión al ir más allá de la correlación (Pearson, Spearman, etc.) o las diferencias de valores centrales (t test, kruska wallis, etc.).

## Preparación de los datos

```{r}
library(rio) # Importamos la data:
hsb = import("https://github.com/luccemhu/EDIT-Clases-E2/raw/main/SESI%C3%93N%201/hsb_ok.xlsx")
#str(hsb) # Tipos de datos reconocidos por R, ya que...
# Todo software lee la data a su modo y no siempre es la que se necesita.
# Por ello, debemos saber qué significa cada columna, su valor, etc.
# Para ello, es importante el codebook o diccionario de datos o metadata
# o el manual metodológico, etc. (Indicarlo en el trabajo).
```

- Formateamos:
```{r}

categoricals = c("SEX", "RACE", "SES", "SCTYP", "HSP", "CAR")

hsb[, categoricals] = lapply(hsb[, categoricals], as.factor) # A factor

# nominales
hsb$SEX = factor(hsb$SEX,
                 levels = c(1, 2),
                 labels = c("Male", "Female"))

hsb$RACE = factor(hsb$RACE,
                  levels = c(1, 2, 3, 4),
                  labels = c("Hispanic", "Asian", "Black", "White"))

hsb$HSP = factor(hsb$HSP,
                 levels = c(1, 2, 3),
                 labels = c("General", "Academic", "Vocational"))

hsb$SCTYP = factor(hsb$SCTYP,
                   levels = c(1, 2),
                   labels = c("Public", "Private"))

# a ordinal:
hsb$SES = ordered(hsb$SES,
                  levels = c(1, 2, 3),
                  labels = c("Low", "Medium", "High"))

#hsb #Ahora veamos la data formateada
```

## Correlación:

### La variable de interés es la V. dependiente: MATH (desempeño en matemáticas)

- Consideremos que nos interesa saber la posible relacion de WRTG en MATH

### 1. V. independiente: WRTG (desempeño en escritura)

- Ya que son `dos` variables numéricas: usaremos una correlación:
  
```{r}
# Gráfica de correlación: (2 dimensiones)
library(ggplot2)
base = ggplot(data = hsb, aes(x = WRTG, y = MATH))
base + geom_point() # Hay aparente relación:
```

###  Índices de correlación:
```{r eval=FALSE}
# despues de la virgulilla: asimétricas, correlacionadas pero ninguna es V.D. o V.I:
f1 = formula( ~ MATH + WRTG) 

# camino parametrico: 
# (a través de ciertas pruebas, se asume correlacion lineal, es decir, 
# las variables presentan disitribución normal)
pearsonf1 = cor.test(f1, data = hsb)[c('estimate', 'p.value')]
pearsonf1
# el coeficiente de Pearson 0.6326664 (con p-value= 0).

# camino no parametrico: (Calcula de la corrleacion en general) 
spearmanf1 = cor.test(f1, data = hsb, 
                      method = 'spearman')[c('estimate', 'p.value')]
spearmanf1
# Spearman:0.6415126 (con p-value= 0).
```

### 2. V. independiente: SCI (desempeño en ciencias)

Como es otra variable (númerica), no podemos calcular la correlacion de 3 variables (No hay pearson o spearman, solo funciona con 2 variables), pero sí se puede ver visualmente (Figura 10).

- Nota: *no se ejecutarán los chunks, solo se muestra el código con esta V.I*:
```{r, eval=FALSE}
# Gráfica de Correlación utilizando el código (`base`) de la primera gráfica:
base + geom_point(aes(color = SCI))
# Visualmente hay relación, pero no tenemos un coeficiente para medirlo:
```

###  Índices de correlación:
```{r, eval=FALSE}
f2 = formula( ~ MATH + SCI)

# camino parametrico
pearsonf2 = cor.test(f2, data = hsb)[c('estimate', 'p.value')]
#pearsonf2 # El coeficiente de Pearson (0.6495261, p-value= 0)

# camino no parametrico
spearmanf2 = cor.test(f2, data = hsb, method = 'spearman')[c('estimate', 'p.value')]
#spearmanf2 # El coeficiente de Spearman (0.6551515,p-value= 0)



```


### 3. V. independiente: SEX (Categorica, dicotomica)

- ¿Influencia del Sex en MATH? (Diferencias de grupos[compararar ])

#### Boxplot con notch (sugiere igualdad de medianas si éstos se intersectan):  
```{r}
# Gráfica:
# Parece no haber diferencia sustantiva entre hombres y mujeres en 
# cuanto a su desempeño en MATH:
base = ggplot(data = hsb, aes(x = SEX, y = MATH))
base + geom_boxplot(notch = T) +  geom_jitter(color = "black",
                                              size = 0.4,
                                              alpha = 0.9)
```

#### Otra mejor opción: Barras de error (si las lineas se instersectan, las medias podrian ser iguales): 
```{r}
library(ggpubr)
ggerrorplot(data = hsb, x = "SEX", y = "MATH")
```

#### ¿Hay o no igualdad de distribuciones en SEX? 

- Depende si las variables se distribuyen o no de manera normal

```{r}
# Los histogramas de la data real tienen encima la curva normal que idealmente
# tendría esa data. La lejanía entre ellos, sugeriría no normalidad.
# Female distribución multimodal (puede afectar la diferencia de grupos)

ggplot(hsb, aes(x = MATH)) +
  geom_histogram(aes(y = ..density..), bins = 20, fill = 'green') +
  stat_function(fun = dnorm,
                colour = "red",
                args = list(mean = mean(hsb$MATH, na.rm = TRUE),
                            sd = sd(hsb$MATH, na.rm = TRUE))) +
  facet_grid( ~ SEX) +
  coord_flip()
```

#### Gráfica qqplot para explorar la presencia/ausencia de normalidad:
```{r}
# Se sugiere normalidad si los puntos no se alejan de la diagonal.
ggqqplot(data = hsb, x = "MATH") + facet_grid(. ~ SEX)
```

#### Como no es facil discenir visualmente la normalidad, debemos calcular algun coeficiente como el Shapiro-Wilk:

```{r}
library(knitr)
library(kableExtra)

f3 = formula(MATH ~ SEX)

tablag = aggregate(f3,
                   hsb,
                   FUN = function(x) {y <- shapiro.test(x);
                   c(y$statistic, y$p.value)})
# Nota:
# Tstudent: se utiliza cuando hay normalidad 
# Prob: es el pvalue y ambas son significativas (menores a 0.05) 
# para Shapiro (parametrico). En suma, no hay normalidad:

# para que se vea mejor:
shapiroTest = as.data.frame(tablag[, 2])
names(shapiroTest) = c("W", "Prob")

kable(cbind(tablag[1], shapiroTest)) |> 
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = F,
                position = "left")
```

#### Se debe realizar una prueba no parametrica para advertir la diferencia de valores medios: usar la prueba Mann-Whitney en vez de la prueba t para testaer la relación entre ambas.
```{r}
# Prueba t:
#tf3=t.test(f3,data=hsb)[c('estimate','p.value')]
#tf3

# Corresponde realizar la prueba no paramétrica:
wilcoxf3=wilcox.test(f3,data=hsb)['p.value']
# No se rechazaría la igualdad de valores medios 
# (Mann-Whitney con p valor = 0.3085543).
#wilcoxf3 
```

#### Gráfica de correlación representando al Sex entre WRTG y MATH:
```{r}
base = ggplot(data = hsb, aes(x = WRTG, y = MATH))
base + geom_point(aes(color = SEX))
```

#### Si añadimos SCI:
```{r}
base + geom_point(aes(size = SCI, color = SEX)) 
```

#### Añadimos SCI (Otro modo):
```{r, eval=FALSE}
base + geom_point(aes(color = SCI)) + facet_grid( ~ SEX)
```

#### En 3 dimensiones:
```{r, fig.cap="Figure 1: Figura 10: 'SCI', 'WRTG', 'MATH'"}
paleta <- c("coral1", "cyan")
colors <- paleta[as.numeric(hsb$SEX)]
library(scatterplot3d)

scatterplot3d(hsb[, c('SCI', 'WRTG', 'MATH')], color = colors)
```

- En suma, los hombres y las mujeres están distribuidos por todo el gráfico, es decir, no hay diferencias aun en dimensiones mayores a 2. Pero no sabemos cuánto afecta cada VI a la VD. Por tanto, necesitamos la regresión...

II. Regresión Lineal...
============================================================

- Ténica en el que hay que definir una V.D. y una o más V.I. (Estas pueden ser predictores, pero por defecto son explicativos: VI explicaría a la VD: representado a traves de un modelo (ecuación)

- Informa cuánto una V.I. explica la variación de una V.D. Por tanto, es una técnica para probar hipótesis direccionales o asimétricas (las correlaciones tienen hipótesis simétricas)

Establezcamos los 3 modelos:
```{r}
modelo1 = formula(MATH ~ WRTG)
modelo2 = formula(MATH ~ WRTG + SCI)
modelo3 = formula(MATH ~ WRTG + SCI + SEX)
```

## H1: el nivel de desempeño en escritura afecta el desempeño en matemáticas:
```{r}
library(stargazer)
reg1 = lm(modelo1, data = hsb)
#summary(reg1)
stargazer(reg1, type = "text", intercept.bottom = FALSE)
```
Interpretación: 

WRTG <- 0.612*** <- Estimate       
         (0.031) <- Std. Error

1. WRTG es significativo (***) y explica a MATH. 

2. Efecto directo (coeficiente directo)

3. La magnitud del efecto es 0.612: Si WRTG aumenta en una unidad, MATH aumentará en promedio 0.612. 

### Recta sobre el gráfico de correlación produce una linea:
```{r, eval=FALSE}
ggplot(hsb, aes(x = WRTG, y = MATH)) +
  geom_point() +
  geom_smooth(method = lm) # Recta
```

- Relacion de estas variables en una ecuación:

Esa recta podemos representarla así:

$$  MATH= `r reg1$coefficients[1]` + `r reg1$coefficients[2]` \cdot WRTG + \epsilon$$

El Y verdadero es MATH, pero la regresión produce un $\hat{MATH}$ estimado, de ahi la presencia del $\epsilon$. Justamente el _R cuadrado ajustado_ (`r summary(reg1)$r.squared`) nos brinda un porcentaje (multiplicalo por 100) que da una pista de nuestra cercanía a una situación perfecta (cuando vale **1**).


## H2: Añadimos SCI:

```{r}
reg2 = lm(modelo2, data = hsb)
stargazer(reg2, type = "text", intercept.bottom = FALSE)
```

### La gráfica produce un plano (no una linea)...

pues la regresion presenta una formula cons 2 V.I. explicando la D.V.

```{r eval=FALSE}
G  <- scatterplot3d(hsb[, c('SCI', 'WRTG', 'MATH')])
G$plane3d(reg2, draw_polygon = TRUE, draw_lines = FALSE)
```

La ecuación del plano:

$$  MATH= `r reg2$coefficients[1]` + `r reg2$coefficients[2]` \cdot WRTG + `r reg2$coefficients[3]` \cdot SCI + \epsilon$$

En este caso el _R cuadrado ajustado_ (`r summary(reg2)$r.squared`) nos da una pista de nuestra lejanía a una situación perfecta.

El coeficiente de WRTG ha variado en la fórmula ahora que está presente SCI debido a que en el primer caso, WRTG y $\epsilon$ buscaban representar la variabilidad en MATH, y ahora, en el segundo caso, viene SCI para mejorar esa explicación; así  el peso de la explicación se recalcula y el coeficiente de WRTG deja de explicar lo que le corresponde a SCI, y $\epsilon$ también le entrega _algo_ a SCI. 

Como $\epsilon$ no tiene coeficiente, representamos su variación usando el error típico de los residuos o _residual standard error_ (RSE). Este ha variado de un modelo ha otro, ahora es un RSE menor. 

### Para comprobar si esta disminución del error es significativa:
```{r}
tanova = anova(reg1, reg2)
tanova
stargazer(tanova,
          type = 'text',
          summary = F,
          title = "Table de Análisis de Varianza")
```

La comparación de modelos usando la tabla de análisis de varianza (anova) propone como hipótesis nula que los modelos no difieren (no se ha reducido el error al pasar de un modelo a otro). Como la comparación es significativa (vea el Pr(> F): menor a 0.05), rechazamos igualdad de modelos: el modelo 2 sí reduce el error al incluir una variable más.

## H3: Añadiendo SEX
```{r}
reg3 = lm(modelo3, data = hsb)
stargazer(reg3, type = "text", intercept.bottom = FALSE)
```

### Gráfica (elementos visuales):

- no se puede graficar 4 coordenadas
```{r, eval=FALSE}
colors <- paleta[as.numeric(hsb$SEX)]
G  <- scatterplot3d(hsb[, c('SCI', 'WRTG', 'MATH')], color = colors)
G$plane3d(reg2, draw_polygon = TRUE, draw_lines = FALSE)
```

Nuestra nueva ecuación sería:

$$  MATH= `r reg3$coefficients[1]` + `r reg3$coefficients[2]` \cdot WRTG + `r reg3$coefficients[3]` \cdot SCI + `r reg3$coefficients[4]` \cdot SEX + \epsilon$$

Nuevamente podemos ver si añadir SEXO en este modelo representa una mejora al anterior:



## Resumiendo las 3 regresiones:
```{r}
#stargazer(reg1, reg2, reg3, type = "text") # Tabla básica

stargazer(reg1, reg2, reg3, type = "text", # Tabla fachera
          title = "Modelos planteados",
          digits = 2,
          single.row = F,
          no.space = F,
          intercept.bottom = FALSE,
          dep.var.caption = "Variable dependiente:",
          dep.var.labels = "Desempeño en Matemáticas",
          covariate.labels = c("Constante",
                               "Desempeño en Escritura",
                               "Desempeño en Ciencias",
                               "SEXO (mujer)"),
          keep.stat = c("n", "adj.rsq", "ser"),
          df = F,
          notes.label = "Notas:")
```

## Gráficamente las 3 regresiones:
```{r}
library(sjPlot)
plot_models(reg1,
            reg2,
            reg3,
            vline.color = "grey",
            m.labels = c("Modelo 1", "Modelo 2", "Modelo 3"))
```

- Ninguna de las variables (y sus intervalos de confianza) tocan el valor cero (que significa que la variable no tiene efecto en la dependiente).

[Comienzo](#beginning)