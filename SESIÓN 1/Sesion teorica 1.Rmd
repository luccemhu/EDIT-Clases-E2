---
title: "Regresión Lineal Multivariada (I)"
subtitle: 'Repaso 1 - Introducción'
date: "2023-07-24"
author: "Gianfranco Romero & Joel Hu | \n **[@GianfrancoRomero](https://github.com/GianfrancoRomero)** \n a20196091@pucp.edu.pe & \n **[@luccemhu](https://github.com/luccemhu)** \n a20196510@pucp.edu.pe"
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
    self_contained: true
    thumbnails: false
    lightbox: true
    gallery: false
    highlight: github
    code_folding: "show"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

I. De Correlación a regresión
============================================================

- Utilizaremos la técnica de la regresión al ir más allá de la correlación (Pearson, Spearman, etc.) o las diferencias de valores centrales (t test, kruska wallis, etc.).

```{r}
library(rio) # Importamos la data:
hsb = import("https://github.com/luccemhu/EDIT-Clases-E2/raw/main/SESI%C3%93N%201/hsb_ok.xlsx")
#str(hsb) # Tipos de datos reconocidos por R, ya que...
# Todo software lee la data a su modo y no siempre es la que se necesita.
# Por ello, debemos saber qué significa cada columna, su valor, etc.
# Para ello, es importante el codebook o diccionario de datos o metadata
# o el manual metodológico, etc. (Indicarlo en el trabajo).
```

## Preparación de los datos
```{r}
# Formateamos:
categoricals = c("SEX", "RACE", "SES", "SCTYP", "HSP", "CAR")

hsb[, categoricals] = lapply(hsb[, categoricals], as.factor) # A factor

# nominales
hsb$SEX = factor(hsb$SEX,
                 levels = c(1, 2),
                 labels = c("Male", "Female"))

hsb$RACE = factor(hsb$RACE,
                  levels = c(1, 2, 3, 4),
                  labels = c("Hispanic", "Asian", "Black", "White"))

hsb$HSP = factor(hsb$HSP,
                 levels = c(1, 2, 3),
                 labels = c("General", "Academic", "Vocational"))

hsb$SCTYP = factor(hsb$SCTYP,
                   levels = c(1, 2),
                   labels = c("Public", "Private"))

# a ordinal:
hsb$SES = ordered(hsb$SES,
                  levels = c(1, 2, 3),
                  labels = c("Low", "Medium", "High"))

#hsb #Ahora veamos la data formateada
```

## Correlación:

### La variable de interés es la V. dependiente: MATH (desempeño en matemáticas)

- Consideremos que nos interesa saber la posible relacion de WRTG en MATH

### 1. V. independiente: WRTG (desempeño en escritura)

- Ya que son `dos` variables numéricas: usaremos una correlación:
  
```{r}
# Gráfica de correlación: (2 dimensiones)
library(ggplot2)
base = ggplot(data = hsb, aes(x = WRTG, y = MATH))
base + geom_point() # Hay aparente relación:
```

###  Índices de correlación:
```{r eval=FALSE}
# despues de la virgulilla: asimétricas, correlacionadas pero ninguna es V.D. o V.I:
f1 = formula( ~ MATH + WRTG) 

# camino parametrico: 
# (a través de ciertas pruebas, se asume correlacion lineal, es decir, 
# las variables presentan disitribución normal)
pearsonf1 = cor.test(f1, data = hsb)[c('estimate', 'p.value')]
pearsonf1
# el coeficiente de Pearson 0.6326664 (con p-value= 0).

# camino no parametrico: (Calcula de la corrleacion en general) 
spearmanf1 = cor.test(f1, data = hsb, 
                      method = 'spearman')[c('estimate', 'p.value')]
spearmanf1
# Spearman:0.6415126 (con p-value= 0).
```

### 2. V. independiente: SCI (desempeño en ciencias)

Como es otra variable (númerica), no podemos calcular la correlacion de 3 variables (No hay pearson o spearman, solo funciona con 2 variables), pero sí se puede ver visualmente (Figura 10).

- Nota: *no se ejecutarán los chunks, solo se muestra el código con esta V.I*:
```{r, eval=FALSE}
# Gráfica de Correlación utilizando el código de la primera gráfica:
base + geom_point(aes(color = SCI))
# Visualmente hay relación, pero no tenemos un coeficiente para medirlo:
```

###  Índices de correlación:
```{r, eval=FALSE}
f2 = formula( ~ MATH + SCI)

# camino parametrico
pearsonf2 = cor.test(f2, data = hsb)[c('estimate', 'p.value')]
# El coeficiente de Pearson (0.6495261, p-value= 0)

# camino no parametrico
spearmanf2 = cor.test(f2, data = hsb, method = 'spearman')[c('estimate', 'p.value')]
# El coeficiente de Spearman (0.6551515,p-value= 0)

pearsonf2
spearmanf2
```


### 3. V. independiente: SEX (Categorica, dicotomica)

- ¿Influencia del Sex en MATH? (Diferencias de grupos[compararar ])

#### Boxplot con notch (sugiere igualdad de medianas si éstos se intersectan):  
```{r}
# Gráfica:
base = ggplot(data = hsb, aes(x = SEX, y = MATH))
base + geom_boxplot(notch = T) +  geom_jitter(color = "black",
                                              size = 0.4,
                                              alpha = 0.9)
# Parece no haber diferencia sustantiva entre hombres y mujeres en 
# cuanto a su desempeño en MATH:
```

#### Otra mejor opción: Barras de error (si las lineas se instersectan, las medias podrian ser iguales): 
```{r}
library(ggpubr)
ggerrorplot(data = hsb, x = "SEX", y = "MATH")
```

#### ¿Hay o no igualdad de distribuciones en SEX? 

- Depende si las variables se distribuyen o no de manera normal

```{r}
ggplot(hsb, aes(x = MATH)) +
  geom_histogram(aes(y = ..density..), bins = 20, fill = 'green') +
  stat_function(fun = dnorm,
                colour = "red",
                args = list(mean = mean(hsb$MATH, na.rm = TRUE),
                            sd = sd(hsb$MATH, na.rm = TRUE))) +
  facet_grid( ~ SEX) +
  coord_flip()
# Los histogramas de la data real tienen encima la curva normal que idealmente tendría esa data. La lejanía entre ellos, sugeriría no normalidad.
# Female distribución multimodal (puede afectar la diferencia de grupos)
```

- Gráfica qqplot para explorar la presencia/ausencia de normalidad:
```{r}
# Se sugiere normalidad si los puntos no se alejan de la diagonal.
ggqqplot(data = hsb, x = "MATH") + facet_grid(. ~ SEX)
```

- Como no es facil discenir visualmente la normalidad, debemos calcular algun coeficiente como el Shapiro-Wilk:

```{r}
library(knitr)
library(kableExtra)

f3 = formula(MATH ~ SEX)

tablag = aggregate(f3,
                   hsb,
                   FUN = function(x) {y <- shapiro.test(x);
                   c(y$statistic, y$p.value)})

# para que se vea mejor:

shapiroTest = as.data.frame(tablag[, 2])
names(shapiroTest) = c("W", "Prob")


kable(cbind(tablag[1], shapiroTest)) |> 
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = F,
                position = "left")
# Tstudent: se utiliza cuando hay normalidad 
# Prob: es el pvalue y ambas son significativas (menores a 0.05) para Shapiro (parametrico)
# En suma, no hay normalidad:

```

- Se debe realizar una prueba no parametrica para advertir la diferencia de valores medios: usar la prueba Mann-Whitney en vez de la prueba t para testaer la relación entre ambas.
```{r}
# Prueba t:
#tf3=t.test(f3,data=hsb)[c('estimate','p.value')]
#tf3

# Prueba no paramétrica:
wilcoxf3=wilcox.test(f3,data=hsb)['p.value']
# No se rechazaría la igualdad de valores medios 
# (Mann-Whitney con p valor = 0.3085543).
wilcoxf3 
```


```{r}
base = ggplot(data = hsb, aes(x = WRTG, y = MATH))
base + geom_point(aes(color = SEX))
```
  - Añadimos SCI:
```{r}
base + geom_point(aes(size = SCI, color = SEX)) 
```
  - Añadimos SCI (Otro modo):
```{r, eval=FALSE}
base + geom_point(aes(color = SCI)) + facet_grid( ~ SEX)
```

  - En 3 dimensiones:
```{r, fig.cap="Figure 1: Figura 10: 'SCI', 'WRTG', 'MATH'"}
paleta <- c("coral1", "cyan")
colors <- paleta[as.numeric(hsb$SEX)]
library(scatterplot3d)

scatterplot3d(hsb[, c('SCI', 'WRTG', 'MATH')], color = colors)
```



II. Regresión Lineal
============================================================

```{r}
modelo1 = formula(MATH ~ WRTG)
modelo2 = formula(MATH ~ WRTG + SCI)
modelo3 = formula(MATH ~ WRTG + SCI + SEX)
```

```{r}
# H1: el nivel de desempeño en escritura afecta el desempeño en matemáticas:
summary(lm(modelo1, data = hsb))
```

```{r}
library(stargazer)
reg1 = lm(modelo1, data = hsb)
summary(reg1)
stargazer(reg1, type = "text", intercept.bottom = FALSE)
```


```{r}
ggplot(hsb, aes(x = WRTG, y = MATH)) +
  geom_point() +
  geom_smooth(method = lm)
```


```{r}
reg2 = lm(modelo2, data = hsb)
summary(reg2)
stargazer(reg2, type = "text", intercept.bottom = FALSE)
```

```{r}
G  <- scatterplot3d(hsb[, c('SCI', 'WRTG', 'MATH')])
G$plane3d(reg2, draw_polygon = TRUE, draw_lines = FALSE)
```

```{r}
tanova = anova(reg1, reg2)
tanova
stargazer(tanova,
          type = 'text',
          summary = F,
          title = "Table de Análisis de Varianza")
```

```{r}
reg3 = lm(modelo3, data = hsb)
summary(reg3)
```

```{r}
colors <- paleta[as.numeric(hsb$SEX)]
G  <- scatterplot3d(hsb[, c('SCI', 'WRTG', 'MATH')], color = colors)
G$plane3d(reg2, draw_polygon = TRUE, draw_lines = FALSE)
```

```{r}
stargazer(reg1, reg2, reg3, type = "text")
stargazer(reg1,
          reg2,
          reg3,
          type = "text",
          title = "Modelos planteadas",
          digits = 2,
          single.row = F,
          no.space = F,
          intercept.bottom = FALSE,
          dep.var.caption = "Variable dependiente:",
          dep.var.labels = "Desempeño en Matemáticas",
          covariate.labels = c("Constante",
                               "Desempeño en Escritura",
                               "Desempeño en Ciencias",
                               "SEXO (mujer)"),
          keep.stat = c("n", "adj.rsq", "ser"),
          df = F,
          notes.label = "Notas:")
```

```{r}
library(sjPlot)
plot_models(reg1,
            reg2,
            reg3,
            vline.color = "grey",
            m.labels = c("Modelo 1", "Modelo 2", "Modelo 3"))
```





