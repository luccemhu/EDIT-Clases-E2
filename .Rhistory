})
# ahora a numerico
demo[, -c(2, 7)] = lapply(demo[, -c(2, 7)], as.numeric)
View(demo)
# cambiar coma en los decimales:
demo[, -1] = lapply(demo[, -1],
function(x) {
gsub(",", ".", x)
})
# ahora a numerico
demo[, -1] = lapply(demo[, -1], as.numeric)
View(demo)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
rm(list = ls())
library(htmltab)
# links
WhereIDH=list(page="https://es.wikipedia.org/wiki/Anexo:Pa%C3%ADses_por_%C3%ADndice_de_desarrollo_humano",
xpath='//*[@id="mw-content-text"]/div[1]/table[6]') # Esto se actualiza
WhereDEMO=list(page="https://es.wikipedia.org/wiki/%C3%8Dndice_de_democracia",
xpath='//*[@id="mw-content-text"]/div[1]/div[2]/div/table/tbody') # Esto se actualiza
#carga
idh  = htmltab(doc = WhereIDH$page,
which  = WhereIDH$xpath,
encoding = "UTF-8")
demo  = htmltab(doc = WhereDEMO$page,
which  = WhereDEMO$xpath,
encoding = "UTF-8")
names(idh)
names(demo)
# en IDH
## cambio total
newNames = c('Pais', 'EsperanzaVida', 'EscolaridadDuracion', 'EscolaridadPromedio',
'PBI')
names(idh) = newNames
# en DEMO
## Capitalizar
library(stringr)
names(demo) = str_to_title(names(demo))
library(tidyverse)
demo = demo |> select(2:8)
## sin tildes ni ñs.
library(stringi)
names(demo) = stri_trans_general(str = names(demo),
id = "Latin-ASCII")
## sin espacios
names(demo) = gsub(" ", "", names(demo))
idh[, ] = lapply(idh[, ], trimws, whitespace = "[\\h\\v]")
demo[, ] = lapply(demo[, ], trimws, whitespace = "[\\h\\v]")
str(idh)
str(demo)
View(demo)
# cambiar coma en los decimales:
demo[, -1] = lapply(demo[, -1],
function(x) {
gsub(",", ".", x)
})
# ahora a numerico
demo[, -1] = lapply(demo[, -1], as.numeric)
View(idh)
idh[!complete.cases(idh),]
demo[!complete.cases(demo),]
##
idh[idh$Pais=='Camerún','EscolaridadDuracion']=13.1
demo[demo$Puesto==48 & !is.na(demo$Puesto),'Pais']='Panama'
##
idh=idh[complete.cases(idh),]
demo=demo[complete.cases(demo),]
# sin tildes
idh$Pais=stri_trans_general(str = idh$Pais,
id = "Latin-ASCII")
demo[,c(2,9)]=lapply(demo[,c(2,9)],
stri_trans_general,
id = "Latin-ASCII")
# sin tildes
idh$Pais=stri_trans_general(str = idh$Pais,
id = "Latin-ASCII")
demo[,c(2,7)]=lapply(demo[,c(2,7)],
stri_trans_general,
id = "Latin-ASCII")
setdiff(demo$Pais,idh$Pais)
setdiff(idh$Pais,demo$Pais)
demo[demo$Pais=='Republica de China','Pais']='China'
demo[demo$Pais=='R. Democratica del Congo','Pais']='Republica Democratica del Congo'
idhdemo=merge(idh,demo)
head(idhdemo)
View(idhdemo)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
rm(list = ls())
library(htmltab)
# links
WhereIDH=list(page="https://es.wikipedia.org/wiki/Anexo:Pa%C3%ADses_por_%C3%ADndice_de_desarrollo_humano",
xpath='//*[@id="mw-content-text"]/div[1]/table[6]') # Esto se actualiza
WhereDEMO=list(page="https://es.wikipedia.org/wiki/%C3%8Dndice_de_democracia",
xpath='//*[@id="mw-content-text"]/div[1]/div[2]/div/table/tbody') # Esto se actualiza
#carga
idh  = htmltab(doc = WhereIDH$page,
which  = WhereIDH$xpath,
encoding = "UTF-8")
demo  = htmltab(doc = WhereDEMO$page,
which  = WhereDEMO$xpath,
encoding = "UTF-8")
names(idh)
names(demo)
# en IDH
## cambio total
newNames = c('Pais', 'EsperanzaVida', 'EscolaridadDuracion', 'EscolaridadPromedio',
'PBI')
names(idh) = newNames
# en DEMO
## Capitalizar
library(stringr)
names(demo) = str_to_title(names(demo))
library(tidyverse)
demo = demo |> select(2:8)
## sin tildes ni ñs.
library(stringi)
names(demo) = stri_trans_general(str = names(demo),
id = "Latin-ASCII")
## sin espacios
names(demo) = gsub(" ", "", names(demo))
idh[, ] = lapply(idh[, ], trimws, whitespace = "[\\h\\v]")
demo[, ] = lapply(demo[, ], trimws, whitespace = "[\\h\\v]")
str(idh)
str(demo)
# eliminar coma en los miles:
#idh$PBI = gsub(',', '', idh$PBI)
# ahora a numerico
idh[, -1] = lapply(idh[, -1], as.numeric)
# cambiar coma en los decimales:
demo[, -1] = lapply(demo[, -1],
function(x) {
gsub(",", ".", x)
})
# ahora a numerico
demo[, -1] = lapply(demo[, -1], as.numeric)
idh[!complete.cases(idh),]
demo[!complete.cases(demo),]
##
idh[idh$Pais=='Camerún','EscolaridadDuracion']=13.1
demo[demo$Puesto==48 & !is.na(demo$Puesto),'Pais']='Panama'
##
idh=idh[complete.cases(idh),]
demo=demo[complete.cases(demo),]
# sin tildes
idh$Pais=stri_trans_general(str = idh$Pais,
id = "Latin-ASCII")
demo[,c(2,7)]=lapply(demo[,c(2,7)],
stri_trans_general,
id = "Latin-ASCII")
setdiff(demo$Pais,idh$Pais)
setdiff(idh$Pais,demo$Pais)
demo[demo$Pais=='Republica de China','Pais']='China'
demo[demo$Pais=='R. Democratica del Congo','Pais']='Republica Democratica del Congo'
idhdemo=merge(idh,demo)
head(idhdemo)
dontselect = c("Pais", "Puesto", "Puntuacion", 'Categoria')
select = setdiff(names(idhdemo), dontselect)
theData = idhdemo[, select]
# Polycor halla la relación policórica: funciona con variables categóricas y numéricas
# Cuando solo se usa con numéricas, calcula el pearson, la correlación.
library(polycor)
corMatrix=polycor::hetcor(theData)$correlations
View(theData)
#########################################
#                 Mapping poverty with R
#                 Milos Popovic
#                 2023/07/25
#########################################
install.packages("remotes")
install.packages("remotes")
#########################################
#                 Mapping poverty with R
#                 Milos Popovic
#                 2023/07/25
#########################################
install.packages("remotes")
remotes::github_install(
"dickoa/rgeoboundaries"
)
install.packages("remotes")
libs <- c(
"tidyverse", "rgeoboundaries",
"sf", "terra", "rayshader"
)
installed_libs <- libs %in% rownames(
installed.packages()
if (any(installed_libs == F)) {
installed_libs <- libs %in% rownames(
installed.packages()
) if (any(installed_libs == F)) {
if(any(installed_libs == F)) {
install.packages(
libs[!installed_libs]
)
}
installed_libs <- libs %in% rownames(
installed.packages()
if(any(installed_libs == F)) {
if(any(installed_libs == F)) {
install.packages(
libs[!installed_libs]
)
}
libs <- c(
"tidyverse", "rgeoboundaries",
"sf", "terra", "rayshader"
)
installed_libs <- libs %in% rownames(
installed.packages()
)
if(any(installed_libs == F)) {
install.packages(
libs[!installed_libs]
)
}
invisible(
lapply(
libs, library,
character.only = T
)
)
install.packages("rgeoboundaries")
install.packages("remotes")
remotes::github_install(
"dickoa/rgeoboundaries"
)
install.packages("remotes")
libs <- c(
"tidyverse", "rgeoboundaries",
"sf", "terra", "rayshader"
)
installed_libs <- libs %in% rownames(
installed.packages()
)
if(any(installed_libs == F)) {
install.packages(
libs[!installed_libs]
)
}
invisible(
lapply(
libs, library,
character.only = T
)
)
remotes::github_install("dickoa/rgeoboundaries")
install.packages("remotes")
install.packages("remotes")
remotes::github_install("dickoa/rgeoboundaries")
remotes::install_gitlab("dickoa/rgeoboundaries")
remotes::install_github("wmgeolab/rgeoboundaries")
remotes::install_github("wmgeolab/rgeoboundaries")
remotes::github_install("dickoa/rgeoboundaries")
Cumulative Var 0.393 0.763 --> La varianza acumulada (suma de las varianzas acumuladas:): me quedo con 76% de informacion de reducir las 9 dimensiones a 2.
# borrando todo:
rm(list = ls())
### links
linkEDUgdp="https://www.cia.gov/the-world-factbook/field/education-expenditures/country-comparison"
linkMILIgdp="https://www.cia.gov/the-world-factbook/field/military-expenditures/country-comparison"
linkKWHprod="https://www.cia.gov/the-world-factbook/field/electricity-production/country-comparison"
### paths
EDUpath='//*[@id="index-content-section"]/div/div[2]/div/div/div/div/div/table'
MILIpath='//*[@id="index-content-section"]/div/div[2]/div/div/div/div/div/table'
KWHpath = '//*[@id="index-content-section"]/div/div[2]/div/div/div/div/div/table'
### scrapping
library(htmltab)
edu<- htmltab(doc = linkEDUgdp,
which =EDUpath)
rm(list = ls())
link_idhdemo="https://docs.google.com/spreadsheets/d/e/2PACX-1vRB4YNe2KdIrTmQUAMScYuWcA2ig8d5fKgJBQIlRPVKcryiurAY3dz4Dy8-fpa_MjqmPeTeYet1ggDR/pub?gid=1870508685&single=true&output=csv"
idhdemo=read.csv(link_idhdemo)
names(idhdemo)
rm(list = ls())
link_idhdemo="https://docs.google.com/spreadsheets/d/e/2PACX-1vRSoEIFbRc_Angw6-if_lje9Ds39pDlp-x7gPz5xcoVCA1ACtcBMnuOvnpPZcCTL7jLCdQ03kIZgSkR/pub?gid=0&single=true&output=csv"
idhdemo=read.csv(link_idhdemo)
names(idhdemo)
View(idhdemo)
rm(list = ls())
link_idhdemo="https://docs.google.com/spreadsheets/d/e/2PACX-1vRB4YNe2KdIrTmQUAMScYuWcA2ig8d5fKgJBQIlRPVKcryiurAY3dz4Dy8-fpa_MjqmPeTeYet1ggDR/pub?gid=1870508685&single=true&output=csv"
idhdemo=read.csv(link_idhdemo)
names(idhdemo)
View(idhdemo)
rm(list = ls())
link_idhdemo="https://docs.google.com/spreadsheets/d/e/2PACX-1vRB4YNe2KdIrTmQUAMScYuWcA2ig8d5fKgJBQIlRPVKcryiurAY3dz4Dy8-fpa_MjqmPeTeYet1ggDR/pub?gid=1870508685&single=true&output=csv"
idhdemo=read.csv(link_idhdemo)
names(idhdemo)
rm(list = ls())
link_idhdemo="https://docs.google.com/spreadsheets/d/e/2PACX-1vRSoEIFbRc_Angw6-if_lje9Ds39pDlp-x7gPz5xcoVCA1ACtcBMnuOvnpPZcCTL7jLCdQ03kIZgSkR/pub?gid=0&single=true&output=csv"
idhdemo=read.csv(link_idhdemo)
names(idhdemo)
idhdemo[,c(4:7)]=normalize(idhdemo[,c(4:7)],method='standardize')
View(idhdemo)
dataClus=allData[,-1]
library(BBmisc)
idhdemo[,c(4:8)]=normalize(idhdemo[,c(4:8)],method='standardize')
library(BBmisc)
idhdemo[,c(4:8)]=normalize(idhdemo[,c(4:8)],method='standardize')
coridhdemo[,c(4:8)]
cor(idhdemo[,c(4:8)])
dataClus=idhdemo[,c(4:8)]
row.names(dataClus)=allData$Country
dataClus=idhdemo[,c(4:8)]
row.names(dataClus)=idhdemo$country
library(cluster)
g.dist = daisy(dataClus, metric="gower")
## para PAM
library(factoextra)
fviz_nbclust(dataClus, pam,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F)
## PARA JERARQUICO
fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "agnes")
## PARA JERARQUICO
fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "diana") #Se busca el lugar donde la línea baja, y se escoge el valor inmediatamente anteiror a la bajada #En este caso, nos sugiere 5, pero como el valor es muy similar a 4, probamos con 4 clusters
###pam
set.seed(123)
grupos=4
res.pam=pam(g.dist,k = grupos,cluster.only = F)
dataClus$pam=res.pam$cluster
###agnes
res.agnes<- hcut(g.dist, k =grupos,hc_func='agnes',hc_method = "ward.D")
dataClus$agnes=res.agnes$cluster
### diana
res.diana <- hcut(g.dist, k = grupos,hc_func='diana')
dataClus$diana=res.diana$cluster
###pam
set.seed(123)
res.pam=pam(g.dist,k = 5,cluster.only = F)
dataClus$pam=res.pam$cluster
###agnes
res.agnes<- hcut(g.dist, k =3,hc_func='agnes',hc_method = "ward.D")
dataClus$agnes=res.agnes$cluster
### diana
res.diana <- hcut(g.dist, k = 3,hc_func='diana')
dataClus$diana=res.diana$cluster
fviz_silhouette(res.pam) #Los valores negativos indican casos (países) que no deberían estar ahí #Los que son menores pero positivos indican que quizás no debería estar ahí, peor es mejor que este ahí que en otro cluster
fviz_silhouette(res.agnes)
fviz_silhouette(res.diana) #Es mejor que quedarse con la sulieta más ancha. En este caso, el ancho promedio de diana es 0.41, el mayor, por lo que nos quedamos con este método de conglomeración #Además, no tiene valores negativos, a diferencia de los otros dos #Una vez han pasado el filtro del promedio de la silueta y los negativos, podríamos ver que tanto sse acumulan los casos en cierto cluster
library(magrittr)
silPAM=data.frame(res.pam$silinfo$widths) #Recupera los silwith de los gráficos anteirores
silPAM$country=row.names(silPAM)
poorPAM=silPAM[silPAM$sil_width<0,'country']%>%sort() #Saca los países que fueron negativos
silAGNES=data.frame(res.agnes$silinfo$widths)
silAGNES$country=row.names(silAGNES)
poorAGNES=silAGNES[silAGNES$sil_width<0,'country']%>%sort()
silDIANA=data.frame(res.diana$silinfo$widths)
silDIANA$country=row.names(silDIANA)
poorDIANA=silDIANA[silDIANA$sil_width<0,'country']%>%sort()
###
library("qpcR")
mal_Clus=as.data.frame(qpcR:::cbind.na(poorPAM, poorAGNES,poorDIANA))
mal_Clus
intersect(poorPAM,poorAGNES) #Intersección de porrPAM y poorAGNES
# en PAM pero NO en Agnes
setdiff(poorPAM,poorAGNES)
original=aggregate(.~ diana, data=dataClus,mean)
original[order(original$edu_gdp),]
setdiff(poorAGNES,poorPAM)
rm(list = ls())
link_idhdemo="https://docs.google.com/spreadsheets/d/e/2PACX-1vRSoEIFbRc_Angw6-if_lje9Ds39pDlp-x7gPz5xcoVCA1ACtcBMnuOvnpPZcCTL7jLCdQ03kIZgSkR/pub?gid=0&single=true&output=csv"
idhdemo=read.csv(link_idhdemo)
names(idhdemo)
library(BBmisc)
idhdemo[,c(4:8)]=normalize(idhdemo[,c(4:8)],method='standardize')
cor(idhdemo[,c(4:8)])
dataClus=idhdemo[,c(4:8)]
row.names(dataClus)=idhdemo$country
library(cluster)
g.dist = daisy(dataClus, metric="gower")
## para PAM
library(factoextra)
fviz_nbclust(dataClus, pam,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F)
## PARA JERARQUICO
fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "agnes")
## PARA JERARQUICO
fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "diana") #Se busca el lugar donde la línea baja, y se escoge el valor inmediatamente anteiror a la bajada #En este caso, nos sugiere 5, pero como el valor es muy similar a 4, probamos con 4 clusters
###pam
set.seed(123)
res.pam=pam(g.dist,k = 5,cluster.only = F)
dataClus$pam=res.pam$cluster
###agnes
res.agnes<- hcut(g.dist, k =3,hc_func='agnes',hc_method = "ward.D")
dataClus$agnes=res.agnes$cluster
### diana
res.diana <- hcut(g.dist, k = 3,hc_func='diana')
dataClus$diana=res.diana$cluster
fviz_silhouette(res.pam) #Los valores negativos indican casos (países) que no deberían estar ahí #Los que son menores pero positivos indican que quizás no debería estar ahí, peor es mejor que este ahí que en otro cluster
fviz_silhouette(res.agnes)
fviz_silhouette(res.diana) #Es mejor que quedarse con la sulieta más ancha. En este caso, el ancho promedio de diana es 0.41, el mayor, por lo que nos quedamos con este método de conglomeración #Además, no tiene valores negativos, a diferencia de los otros dos #Una vez han pasado el filtro del promedio de la silueta y los negativos, podríamos ver que tanto sse acumulan los casos en cierto cluster
library(magrittr)
silPAM=data.frame(res.pam$silinfo$widths) #Recupera los silwith de los gráficos anteirores
silPAM$country=row.names(silPAM)
poorPAM=silPAM[silPAM$sil_width<0,'country']%>%sort() #Saca los países que fueron negativos
silAGNES=data.frame(res.agnes$silinfo$widths)
silAGNES$country=row.names(silAGNES)
poorAGNES=silAGNES[silAGNES$sil_width<0,'country']%>%sort()
silDIANA=data.frame(res.diana$silinfo$widths)
silDIANA$country=row.names(silDIANA)
poorDIANA=silDIANA[silDIANA$sil_width<0,'country']%>%sort()
###
library("qpcR")
mal_Clus=as.data.frame(qpcR:::cbind.na(poorPAM, poorAGNES,poorDIANA))
mal_Clus
intersect(poorPAM,poorAGNES) #Intersección de porrPAM y poorAGNES
# en PAM pero NO en Agnes
setdiff(poorPAM,poorAGNES)
setdiff(poorAGNES,poorPAM)
View(idhdemo)
View(dataClus)
dataClus=idhdemo[,c(4:8)]
row.names(dataClus)=idhdemo$Country
rm(list = ls())
link_idhdemo="https://docs.google.com/spreadsheets/d/e/2PACX-1vRSoEIFbRc_Angw6-if_lje9Ds39pDlp-x7gPz5xcoVCA1ACtcBMnuOvnpPZcCTL7jLCdQ03kIZgSkR/pub?gid=0&single=true&output=csv"
idhdemo=read.csv(link_idhdemo)
names(idhdemo)
library(BBmisc)
idhdemo[,c(4:8)]=normalize(idhdemo[,c(4:8)],method='standardize')
cor(idhdemo[,c(4:8)])
dataClus=idhdemo[,c(4:8)]
row.names(dataClus)=idhdemo$Country
library(cluster)
g.dist = daisy(dataClus, metric="gower")
## para PAM
library(factoextra)
fviz_nbclust(dataClus, pam,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F)
## PARA JERARQUICO
fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "agnes")
## PARA JERARQUICO
fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "diana") #Se busca el lugar donde la línea baja, y se escoge el valor inmediatamente anteiror a la bajada #En este caso, nos sugiere 5, pero como el valor es muy similar a 4, probamos con 4 clusters
###pam
set.seed(123)
res.pam=pam(g.dist,k = 5,cluster.only = F)
dataClus$pam=res.pam$cluster
###agnes
res.agnes<- hcut(g.dist, k =3,hc_func='agnes',hc_method = "ward.D")
dataClus$agnes=res.agnes$cluster
### diana
res.diana <- hcut(g.dist, k = 3,hc_func='diana')
dataClus$diana=res.diana$cluster
fviz_silhouette(res.pam) #Los valores negativos indican casos (países) que no deberían estar ahí #Los que son menores pero positivos indican que quizás no debería estar ahí, peor es mejor que este ahí que en otro cluster
fviz_silhouette(res.agnes)
fviz_silhouette(res.diana) #Es mejor que quedarse con la sulieta más ancha. En este caso, el ancho promedio de diana es 0.41, el mayor, por lo que nos quedamos con este método de conglomeración #Además, no tiene valores negativos, a diferencia de los otros dos #Una vez han pasado el filtro del promedio de la silueta y los negativos, podríamos ver que tanto sse acumulan los casos en cierto cluster
library(magrittr)
silPAM=data.frame(res.pam$silinfo$widths) #Recupera los silwith de los gráficos anteirores
silPAM$country=row.names(silPAM)
poorPAM=silPAM[silPAM$sil_width<0,'country']%>%sort() #Saca los países que fueron negativos
silAGNES=data.frame(res.agnes$silinfo$widths)
silAGNES$country=row.names(silAGNES)
poorAGNES=silAGNES[silAGNES$sil_width<0,'country']%>%sort()
silDIANA=data.frame(res.diana$silinfo$widths)
silDIANA$country=row.names(silDIANA)
poorDIANA=silDIANA[silDIANA$sil_width<0,'country']%>%sort()
###
library("qpcR")
mal_Clus=as.data.frame(qpcR:::cbind.na(poorPAM, poorAGNES,poorDIANA))
mal_Clus
intersect(poorPAM,poorAGNES) #Intersección de porrPAM y poorAGNES
# en PAM pero NO en Agnes
setdiff(poorPAM,poorAGNES)
setdiff(poorAGNES,poorPAM)
rm(list = ls())
link_idhdemo="https://docs.google.com/spreadsheets/d/e/2PACX-1vRSoEIFbRc_Angw6-if_lje9Ds39pDlp-x7gPz5xcoVCA1ACtcBMnuOvnpPZcCTL7jLCdQ03kIZgSkR/pub?gid=0&single=true&output=csv"
idhdemo=read.csv(link_idhdemo)
names(idhdemo)
library(BBmisc)
idhdemo[,c(4:8)]=normalize(idhdemo[,c(4:8)],method='standardize')
cor(idhdemo[,c(4:8)])
dataClus=idhdemo[,c(4:8)]
row.names(dataClus)=idhdemo$Country
library(cluster)
g.dist = daisy(dataClus, metric="gower")
## para PAM
library(factoextra)
fviz_nbclust(dataClus, pam,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F)
## PARA JERARQUICO
fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "agnes")
## PARA JERARQUICO
fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "diana") #Se busca el lugar donde la línea baja, y se escoge el valor inmediatamente anteiror a la bajada #En este caso, nos sugiere 5, pero como el valor es muy similar a 4, probamos con 4 clusters
###pam
set.seed(123)
res.pam=pam(g.dist,k = 5,cluster.only = F)
dataClus$pam=res.pam$cluster
###agnes
res.agnes<- hcut(g.dist, k =3,hc_func='agnes',hc_method = "ward.D")
dataClus$agnes=res.agnes$cluster
### diana
res.diana <- hcut(g.dist, k = 3,hc_func='diana')
dataClus$diana=res.diana$cluster
fviz_silhouette(res.pam) #Los valores negativos indican casos (países) que no deberían estar ahí #Los que son menores pero positivos indican que quizás no debería estar ahí, peor es mejor que este ahí que en otro cluster
fviz_silhouette(res.agnes)
fviz_silhouette(res.diana) #Es mejor que quedarse con la sulieta más ancha. En este caso, el ancho promedio de diana es 0.41, el mayor, por lo que nos quedamos con este método de conglomeración #Además, no tiene valores negativos, a diferencia de los otros dos #Una vez han pasado el filtro del promedio de la silueta y los negativos, podríamos ver que tanto sse acumulan los casos en cierto cluster
library(magrittr)
silPAM=data.frame(res.pam$silinfo$widths) #Recupera los silwith de los gráficos anteirores
silPAM$country=row.names(silPAM)
poorPAM=silPAM[silPAM$sil_width<0,'country']%>%sort() #Saca los países que fueron negativos
silAGNES=data.frame(res.agnes$silinfo$widths)
silAGNES$country=row.names(silAGNES)
poorAGNES=silAGNES[silAGNES$sil_width<0,'country']%>%sort()
silDIANA=data.frame(res.diana$silinfo$widths)
silDIANA$country=row.names(silDIANA)
poorDIANA=silDIANA[silDIANA$sil_width<0,'country']%>%sort()
###
library("qpcR")
mal_Clus=as.data.frame(qpcR:::cbind.na(poorPAM, poorAGNES,poorDIANA))
mal_Clus
intersect(poorPAM,poorAGNES) #Intersección de porrPAM y poorAGNES
# en PAM pero NO en Agnes
setdiff(poorPAM,poorAGNES)
setdiff(poorAGNES,poorPAM)
View(dataClus)
silPAM=data.frame(res.pam$silinfo$widths)
silPAM$country=row.names(silPAM)
poorPAM=silPAM[silPAM$sil_width<0,'country']%>%sort()
poorPAM
