---
title: "Análisis de conglomerados II"
author: "Gianfranco Raúl Romero Sucapuca"
date: '2022-06-03'
output: html_document
---

# 1. Volvamos a traer los datos de la sesión anterior

```{r}
# borrando todo:
rm(list = ls())

### links
linkEDUgdp="https://www.cia.gov/the-world-factbook/field/education-expenditures/country-comparison"
linkMILIgdp="https://www.cia.gov/the-world-factbook/field/military-expenditures/country-comparison"
linkKWHprod="https://www.cia.gov/the-world-factbook/field/electricity-production/country-comparison"

### paths
EDUpath='//*[@id="index-content-section"]/div/div[2]/div/div/div/div/div/table'
MILIpath='//*[@id="index-content-section"]/div/div[2]/div/div/div/div/div/table'
KWHpath = '//*[@id="index-content-section"]/div/div[2]/div/div/div/div/div/table'

### scrapping

library(htmltab)
edu<- htmltab(doc = linkEDUgdp,
              which =EDUpath)
mili<- htmltab(doc = linkMILIgdp,
              which =MILIpath)
elec<- htmltab(doc = linkKWHprod,
              which =KWHpath)

### subsetting
keep=c(2,3)
edu=edu[,keep]
mili=mili[,keep]
elec=elec[,keep]

### renaming
names(edu)[2]="edu_gdp"
names(mili)[2]="mili_gdp"

### merging
allData=merge(edu,mili)
allData=merge(allData,elec)

##3 formatting
library(readr)
allData[,-1]=lapply(allData[,-1], parse_number)

### transforming
library(BBmisc)
allData[,-1]=BBmisc::normalize(allData[,-1],method='standardize')
allData=allData[complete.cases(allData),]

####descriptivos:
summary(allData)
```

Veamos correlaciones

```{r}
cor(allData[,-1])
```

Cambio de *monotonía*

```{r}
allData$edu_gdp=-1*allData$edu_gdp
```

Preparemos la data para la clusterización

```{r}
dataClus=allData[,-1]
row.names(dataClus)=allData$Country
```

Cálculo de la matriz de distancias:

```{r}
library(cluster)
g.dist = daisy(dataClus, metric="gower")
```

# 2. Proponer cantidad de clusters:

Las siguientes gráficas proponen la cantidad de clusters a solicitar (usando el estadístico *gap*):

```{r}
## para PAM

library(factoextra)
fviz_nbclust(dataClus, pam,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F)
```

```{r}
## PARA JERARQUICO

fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "agnes")
```

```{r}
## PARA JERARQUICO

fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "diana") #Se busca el lugar donde la línea baja, y se escoge el valor inmediatamente anteiror a la bajada #En este caso, nos sugiere 5, pero como el valor es muy similar a 4, probamos con 4 clusters
```

# 3. Evaluemos resultados

Pidamos cuatro grupos:

```{r}
###pam
set.seed(123)
grupos=4
res.pam=pam(g.dist,k = grupos,cluster.only = F)
dataClus$pam=res.pam$cluster

###agnes
res.agnes<- hcut(g.dist, k =grupos,hc_func='agnes',hc_method = "ward.D")
dataClus$agnes=res.agnes$cluster

### diana
res.diana <- hcut(g.dist, k = grupos,hc_func='diana')
dataClus$diana=res.diana$cluster
```

Ahora veamos a cuál le fue mejor:

```{r}
fviz_silhouette(res.pam) #Los valores negativos indican casos (países) que no deberían estar ahí #Los que son menores pero positivos indican que quizás no debería estar ahí, peor es mejor que este ahí que en otro cluster
```

```{r}
fviz_silhouette(res.agnes)
```

```{r}
fviz_silhouette(res.diana) #Es mejor que quedarse con la sulieta más ancha. En este caso, el ancho promedio de diana es 0.41, el mayor, por lo que nos quedamos con este método de conglomeración #Además, no tiene valores negativos, a diferencia de los otros dos #Una vez han pasado el filtro del promedio de la silueta y los negativos, podríamos ver que tanto sse acumulan los casos en cierto cluster
```

Se puede concluir que estos datos fueron mejor clusterizados usando el metodo jerarquico divisivo.

-Encontremos los casos MAL clusterizados (silueta negativa):

```{r}
library(magrittr)

silPAM=data.frame(res.pam$silinfo$widths) #Recupera los silwith de los gráficos anteirores
silPAM$country=row.names(silPAM)
poorPAM=silPAM[silPAM$sil_width<0,'country']%>%sort() #Saca los países que fueron negativos

silAGNES=data.frame(res.agnes$silinfo$widths)
silAGNES$country=row.names(silAGNES)
poorAGNES=silAGNES[silAGNES$sil_width<0,'country']%>%sort()

silDIANA=data.frame(res.diana$silinfo$widths)
silDIANA$country=row.names(silDIANA)
poorDIANA=silDIANA[silDIANA$sil_width<0,'country']%>%sort()

###
library("qpcR") 
mal_Clus=as.data.frame(qpcR:::cbind.na(poorPAM, poorAGNES,poorDIANA))
mal_Clus
```

-Podemos usar teoría de conjuntos para ver qué los casos mal clusterizados en todos las técnicas:

```{r}
intersect(poorPAM,poorAGNES) #Intersección de porrPAM y poorAGNES
```

```{r}
# en PAM pero NO en Agnes
setdiff(poorPAM,poorAGNES)
```

```{r}
setdiff(poorAGNES,poorPAM)
```

# 4. Graficando

Por lo anterior sabemos que usaremos la técnica diana. Verifiquemos las etiquetas:

```{r}
original=aggregate(.~ diana, data=dataClus,mean)
original[order(original$edu_gdp),]
```

```{r}
dataClus$diana=dplyr::recode(dataClus$diana, `3` = 1, `4`=2,`2`=3,`1`=4)
```


```{r}
#proyectando los casos en dos dimensiones:

proyeccion = cmdscale(g.dist, k=2,add = T) # k es la cantidad de dimensiones
dataClus$dim1 <- proyeccion$points[,1]
dataClus$dim2 <- proyeccion$points[,2]
base= ggplot(dataClus,aes(x=dim1, y=dim2,label=row.names(dataClus))) 
base + geom_text(size=2, aes(color=as.factor(diana)))  + labs(title = "DIANA") 
```

















